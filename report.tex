\documentclass[12pt, a4paper]{article} % Assuming 'article' class, adjust if needed

% --- Preamble (Include necessary packages) ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage[a4paper, margin=2.5cm]{geometry} % Adjust margins as needed
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath, amssymb}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{float} % For [H] placement if needed later
\usepackage{csquotes} % Required by biblatex when using babel
\usepackage[backend=bibtex, style=numeric]{biblatex} % changed backend to bibtex for simpler compile (or keep biber if you prefer)
\addbibresource{references.bib} % use plain relative filename; ensure references.bib is in the same folder as report.tex

% --- ADD: Allow underscores in normal text to avoid "Missing $ inserted" errors ---
\usepackage{underscore}

% --- Add these to your Preamble ---
\usepackage{xcolor}
\usepackage{listings}

% Define a custom style for Python code
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{purple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Report_Delaplace_20245433},
    pdfpagemode=FullScreen
}

% --- Custom Title Page Info ---
\newcommand{\coursename}{Module: Algorithms and Combinatorial Thinking}
\newcommand{\academicyear}{2025-2026}
\newcommand{\instructorname}{Dr. Franck Delaplace}
\newcommand{\groupmembers}{Christos Christophoros Mitsakopoulos}

\begin{document}

% --- Title Page ---
\begin{titlepage}
    \centering % Center everything on the title page

    % --- Logos ---
    \begin{minipage}[t]{0.1\textwidth}
        \centering
        \includegraphics[height=2.5cm]{logo_blue.png} % Adjust height as needed
    \end{minipage}
    \hfill % Pushes logos to opposite sides
    \begin{minipage}[t]{0.25\textwidth}
        \centering
        \raisebox{0.7cm}{\includegraphics[height=1.5cm]{logo_purple.png}} % Adjust height as needed
    \end{minipage}

    
    {\Huge \bfseries Metaheuristic approach to the Hamiltonian Path \par}


    \vspace{1cm} % Space between logos and title
    \begin{minipage}[t]{1\textwidth}
        \centering
        \includegraphics[width=12cm]{title_page.png}
        \captionof{figure}{\textit{Visualised with Pyvis, depicts the correct Hamiltonian path through the artificially generated, 50 node graph.}}
    \end{minipage}
    \vspace{0.3cm}

    % --- Course/Module Info ---
    {\Large \coursename \space \academicyear \par}

    \vspace{1cm} % Space before author/group info

    % --- Author/Group Info ---
    {\large \textbf{Written by:} \par}
    \vspace{0.2cm}
    \begin{Large}
    \groupmembers
    \end{Large}
    \par

    \vspace{0.5cm} % Space before instructor info

    % --- Instructor Info ---
    {\large \textbf{Supervised by:} \space \instructorname \par}


    \vfill % Pushes the date to the bottom

    % --- Date ---
    {\large \today \par} % Automatically inserts the compilation date
    % Or use: {\large October 27, 2025 \par} for a fixed date

    %\vspace*{1cm} % Add some space at the bottom

\end{titlepage}

% --- Table of Contents ---
\tableofcontents
\newpage

% --- Introduction ---
\section{Formal Definition and Introduction}

To start with, the Hamiltonian Path problem asks if a graph $G=(V, E)$ contains a path that visits every vertex / node $v \in V$ exactly once. 

Given that the Hamiltonian Path problem is NP-Complete, as the number of vertices of the vertex set $n = |V|$ increases, the computational complexity required to solve the problem via brute force grows factorially ($O(n!)$). Consequently, exact algorithms become computationally intractable for large $n$, necessitating the use of metaheuristics -- like the ones tested in this report. For this exact reason no evaluation of a brute force approach has been considered in the experiments of this report, as the computational and time complexity required is too high to practically implement. This report evaluates three metaheuristic approaches: \textit{Simulated Annealing}, \textit{Tabu Search}, and a \textit{Genetic Algorithm}.

In the context of bioinformatics, specifically \textit{de novo} genome assembly, this problem is interesting. Arranging DNA reads in an acyclic graph can be modelled as finding a Hamiltonian path that maximises read overlap to create a contiguous sequence. Since this is a bioinformatics course, a genetic algorithm appropriated from the course material (much like all other algorithms implemented) was tested to match the theme of genome assembly heuristics.

\bigskip

Find all relevant results / as well as reproduce the experiment using the \texttt{main.py} at the following address: \url{https://github.com/cmitsakopoulos/Delaplace_coursework}. The Python script automates all tests demonstrated in this report and moreover, accepts user arguments through the command line interface; intended for tweaking parameters regarding base graph generation.

\begin{lstlisting}[language=Python, caption=\textit{Usage scenario where the number of nodes for the problem graph are set to 50, with the probability of any two nodes having a connecting edge is 0.15. (This was ran in a Python environment -- ensure you install the dependencies listed in the annex.)}]
(project) chrismitsacopoulos@192 Delaplace_coursework % python3 main.py --nodes 50 --prob 0.15
\end{lstlisting}

\section{Test Environment -- Random Generation of the Problem: Erdős-Rényi Random Graph}

To benchmark the metaheuristic algorithms, an Erdős-Rényi (ER) $G(n,p)$ model was used. In which: 
\begin{itemize}
    \item $n$: The number of vertices in the graph.
    \item $p$: The probability that an edge exists between any two distinct vertices.
\end{itemize}

A larger parameter $p$ will by effect increase the likelihood of finding a Hamiltonian Path, given that the probability of any two nodes having a connecting edge is larger. This was evident when first trying out the \texttt{main.py} Python script, where preliminary tests with a  $p$ of $\approx0.3$ and $n=50$ showed that a Hamiltonian Path was indeed mathematically possible to find; so much so that all algorithms would converge to a zero cost solution (no broken edges).

\begin{lstlisting}[language=Python, caption=\textit{Small snippet of code in which NetworkX in Python can generate an ER graph of chosen parameters, identify that the default options are intended for a "challenging" benchmark for the algorithms. Hardcoding the seed was not a deliberate analytical choice, just to ensure reproducibility of the graph itself -- the algorithms are stochastic too...}]
# n default = 50, p default = 0.1, seed hardcoded to 42
g_exp = nx.erdos_renyi_graph(n=num_nodes, p=prob, seed=SEED)
\end{lstlisting}

\bigskip

To ensure significance of the results, each metaheuristic algorithm was executed for $N=30$ runs. Relying on the Central Limit Theorem, the sampling distribution of the mean approximates a normal distribution as $N \geq 30$, even if the underlying population distribution is non-Gaussian. While increasing $N$ reduces the Standard Error of the Mean (SEM), the precision improves only with the square root of $N$ (i.e., $\text{SEM} = \sigma / \sqrt{N}$). 

\section{Metaheuristic Algorithms: Mode of Action and their Implementation}

\subsection{Objective Function}
All algorithms used aim to identify a permutation $S$ of vertices that minimises the number of broken edges in the path. For a graph $G=(V,E)$ and a candidate path $S = [v_1, v_2, \dots, v_n]$, the cost function $C(S)$ is defined as:

\begin{equation}
    C(S) = (n-1) - \sum_{i=1}^{n-1} \mathbb{I}((v_i, v_{i+1}) \in E)
\end{equation}
Where $\mathbb{I}$ is an indicator function that equals 1 if the edge exists and 0 otherwise. A global optimum is reached when $C(S) = 0$ -- a Hamiltonian Path.

\subsection{Simulated Annealing (SA)}

Simulated Annealing explores solutions by accepting both better or even, worse solutions, based on a probability that decreases over time (referred to as parameter $T$). This logic prevents the algorithm from arriving at a final solution, before it has found the true solution inside the solution space. The probability $P$ -- of accepting --  a new solution $S'$ with cost difference $\Delta C = C(S') - C(S)$ is governed by the Metropolis criterion (see also the Python implementation below):

\begin{equation}
    P(\text{accept}) = 
    \begin{cases} 
    1 & \text{if } \Delta C < 0 \\
    e^{-\frac{\Delta C}{T}} & \text{if } \Delta C \geq 0 
    \end{cases}
\end{equation}

\begin{lstlisting}[language=Python, caption=\textit{Metropolis Criterion: As the loop progresses, the temperature $T$ decays geometrically ($T_{k+1} = 0.985 \cdot T_k$), gradually turning the search into a simple greedy descent (hill climbing)}]
# From main.py: Calculate cost difference
delta = neighbor_cost - current_cost

# Accept if better (delta < 0) OR with probability exp(-delta/T)
if delta < 0 or random.random() < exp(-delta / temp):
    current = neighbor
    current_cost = neighbor_cost
\end{lstlisting}

\subsection{Tabu Search (TS)}
Tabu Search differs from SA by using a deterministic, memory-based approach. It explores the immediate neighbourhood of the current solution and moves to the best available neighbour, even if that neighbour is worse than the current solution.

To prevent cycling (revisiting the same solutions endlessly), the algorithm maintains a \textit{Tabu List}—a short-term memory queue that forbids recently visited solutions for a specific duration (tenure).

\begin{lstlisting}[language=Python, caption=Tabu Search Memory Logic]
# From main.py: Moving to the best candidate not in the Tabu list
if cand not in tabu_list:
    current = cand
    found_move = True
    
# Update memory
tabu_list.append(current)
if len(tabu_list) > tenure:
    tabu_list.pop(0) # Remove oldest entry
\end{lstlisting}

\subsection{Genetic Algorithm (GA)}

The Genetic Algorithm mimics natural selection. Unlike SA and TS, which improve a single solution, GA evolves a population of solutions. The core operator for permutation problems is \textit{Ordered Crossover} (OX1), which is necessary because standard single-point crossover would result in invalid paths (duplicate or missing nodes).

The implementation uses OX1 to preserve the relative ordering of a sub-segment from one parent while filling the remaining slots with genes from the second parent.

\begin{lstlisting}[language=Python, caption=Ordered Crossover (OX1) Implementation]
# From main.py: Preserves sub-segment from parent 1
child[start:end] = p1[start:end]

# Fills remaining slots from parent 2, skipping duplicates
for i in range(size):
    if child[i] is None:
        while p2[current_p2_idx] in child:
            current_p2_idx += 1
        child[i] = p2[current_p2_idx]
\end{lstlisting}

\subsection{Comparative Analysis: Neighbourhood Operators}
A critical finding in the experimental setup was the impact of the mutation operator. The standard \texttt{swap} operator (exchanging two indices) disrupts the adjacency of the path significantly.

In contrast, the \texttt{inversion} (2-Opt) operator reverses a segment of the path. This is mathematically superior for path problems because it preserves the internal adjacency of the reversed segment, only breaking the two edges at the endpoints of the segment. The code allows for switching between these operators to demonstrate this efficiency gap.

\begin{equation}
    \text{Swap}(S) \rightarrow \text{High disruption of edges}
\end{equation}
\begin{equation}
    \text{Inversion}(S) \rightarrow \text{Minimal disruption (2-Opt)}
\end{equation}

This distinction is implemented via the `op_inversion` function in `main.py`  and is the primary driver for convergence in denser graphs.



\end{document}